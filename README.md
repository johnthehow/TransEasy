# BERTET: BERT Explainability Toolkit
## glassbox_bert.py
### Illustration of class variable groups
![glassbox_bert grouping](https://github.com/johnthehow/bertet/blob/master/figure1.png)
### Features
1. The glassbox_bert class returns an instance which encloses both data and model aftering being feed with a sentence
2. Unlike Huggingface BertModel instance, with glassbox_bert you can easily obtain all intermediate representations (44 in total) you can ever imagine, not just Attention and Hidden states.
3. The components in glassbox_bert are highly aligned with the concepts of Transformer encoder as in Vaswani et al. 2017.
4. All components (as class variables) are divied into six groups, namely:
	1. Symbolic: with class variable prefix ***sym_***
	1. Pre-Embedding: with class variable prefix ***preemb_***
	1. Multi-Head Self-Attention: with class variable prefix ***selfattn_***
	1. Add & Norm 1: with class variable prefix ***addnorm1_***
	1. Feedforward: with class variable prefix ***ffnn_***
	1. Add & Norm 2: with class variable prefix ***addnorm2_***
5. The results generated by BertModel Pipeline are stored with class variables with prefix ***pipeline_***
6. The results generated by step-wise manual procedure are called with class variables with prefix ***manual_***

### Example Usage
```python
from bertet import glassbox_bert
instance = glassbox_bert.glassbox_bert('an example sentence here')

```

### Note
1. In query, key and value weight matrices, every 64 rows correspond to a head
2. In query, key and value bias matrices, every 64 items correspond to a head
3. Matrix multiplications are done with XW.T, not other way round
3. The activation function of FFNN is gelu
4. The Softmax in Multi-Head Self-Attention is parameterized with axis=-1

### A complete list of class variables of glassbox_bert
* model
* tokenizer
* pipeline_res
* pipeline_attns
* pipeline_hiddens
* sym_sent
* sym_token_ids
* sym_tokens
* sym_seq_len
* preemb_vocab_emb
* preemb_pos_emb
* preemb_seg_emb
* preemb_sum_emb
* preemb_norm_sum_emb
* selfattn_query_weight
* selfattn_key_weight
* selfattn_value_weight
* selfattn_query_bias
* selfattn_key_bias
* selfattn_value_bias
* selfattn_query_hidden
* selfattn_key_hidden
* selfattn_value_hidden
* selfattn_qkt_hidden
* selfattn_qkt_scale_hidden
* selfattn_qkt_scale_soft_hidden
* selfattn_attention
* selfattn_contvec
* selfattn_contvec_concat
* addnorm1_dense_weight
* addnorm1_dense_bias
* addnorm1_dense_hidden
* addnorm1_add_hidden
* addnorm1_norm_hidden
* ffnn_dense_weight
* ffnn_dense_bias
* ffnn_dense_hidden
* ffnn_dense_act
* addnorm2_dense_weight
* addnorm2_dense_bias
* addnorm2_dense_hidden
* addnorm2_add_hidden
* addnorm2_norm_hidden
* manual_hiddens


